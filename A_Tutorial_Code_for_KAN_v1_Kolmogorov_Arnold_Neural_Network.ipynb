{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n0lSPjpjnvB",
        "outputId": "43b2e450-7531-47c4-efd9-81bbc6d360ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: pdm-backend in /usr/local/lib/python3.10/dist-packages (2.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tqdm pdm-backend"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "metadata": {
        "id": "AkT3rjFqjo6x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the KANLinear class\n",
        "class KANLinear(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A custom linear layer that combines a base linear transformation with spline interpolation.\n",
        "\n",
        "    Args:\n",
        "        in_features (int): Size of each input sample.\n",
        "        out_features (int): Size of each output sample.\n",
        "        grid_size (int, optional): Number of grid points for the spline interpolation. Default is 5.\n",
        "        spline_order (int, optional): Order of the spline used in the interpolation. Default is 3.\n",
        "        scale_noise (float, optional): Scaling factor for the noise added during initialization. Default is 0.1.\n",
        "        scale_base (float, optional): Scaling factor for the base weights initialization. Default is 1.0.\n",
        "        scale_spline (float, optional): Scaling factor for the spline weights initialization. Default is 1.0.\n",
        "        enable_standalone_scale_spline (bool, optional): If True, enables independent scaling for spline weights. Default is True.\n",
        "        base_activation (callable, optional): Activation function applied after the base linear transformation. Default is SiLU.\n",
        "        grid_eps (float, optional): Smoothing parameter for the grid adaptation. Default is 0.02.\n",
        "        grid_range (list, optional): Range of the grid values. Default is [-1, 1].\n",
        "\n",
        "    Attributes:\n",
        "        grid (torch.Tensor): Tensor representing the grid points for spline interpolation.\n",
        "        base_weight (torch.nn.Parameter): Parameter for the base linear transformation.\n",
        "        spline_weight (torch.nn.Parameter): Parameter for the spline interpolation weights.\n",
        "        spline_scaler (torch.nn.Parameter): Parameter for scaling the spline weights, if enabled.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        enable_standalone_scale_spline=True,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        # Calculate grid spacing\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "\n",
        "        # Create a grid with the specified range and spacing\n",
        "        grid = (\n",
        "            (\n",
        "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                + grid_range[0]\n",
        "            )\n",
        "            .expand(in_features, -1)\n",
        "            .contiguous()\n",
        "        )\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        # Initialize weights for the base linear transformation and the spline interpolation\n",
        "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = torch.nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "\n",
        "        # Initialize optional scaling parameter for the spline weights\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = torch.nn.Parameter(\n",
        "                torch.Tensor(out_features, in_features)\n",
        "            )\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Resets the parameters of the layer.\"\"\"\n",
        "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = (\n",
        "                (\n",
        "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
        "                    - 1 / 2\n",
        "                )\n",
        "                * self.scale_noise\n",
        "                / self.grid_size\n",
        "            )\n",
        "            self.spline_weight.data.copy_(\n",
        "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
        "                * self.curve2coeff(\n",
        "                    self.grid.T[self.spline_order : -self.spline_order],\n",
        "                    noise,\n",
        "                )\n",
        "            )\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the B-spline bases for the given input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "\n",
        "        grid: torch.Tensor = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "\n",
        "        # Initialize the bases with the first-order condition\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "\n",
        "        # Iteratively calculate higher-order B-spline bases\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the coefficients of the curve that interpolates the given points.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "\n",
        "        # Solve the linear system to find the spline coefficients\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        result = solution.permute(2, 0, 1)\n",
        "\n",
        "        assert result.size() == (\n",
        "            self.out_features,\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        \"\"\"Return the scaled spline weights, optionally applying a scaling factor.\"\"\"\n",
        "        return self.spline_weight * (\n",
        "            self.spline_scaler.unsqueeze(-1)\n",
        "            if self.enable_standalone_scale_spline\n",
        "            else 1.0\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass through the KANLinear layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, out_features).\n",
        "        \"\"\"\n",
        "        assert x.size(-1) == self.in_features\n",
        "        original_shape = x.shape\n",
        "        x = x.reshape(-1, self.in_features)\n",
        "\n",
        "        # Apply the base linear transformation with activation\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "\n",
        "        # Apply the spline interpolation\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.scaled_spline_weight.view(self.out_features, -1),\n",
        "        )\n",
        "\n",
        "        # Combine the base and spline outputs\n",
        "        output = base_output + spline_output\n",
        "        output = output.reshape(*original_shape[:-1], self.out_features)\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        \"\"\"\n",
        "        Update the grid based on input data, allowing for adaptive grid placement.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "            margin (float, optional): Margin added to the grid boundaries. Default is 0.01.\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "\n",
        "        # Calculate spline bases and interpolate coefficients\n",
        "        splines = self.b_splines(x).permute(1, 0, 2)\n",
        "        orig_coeff = self.scaled_spline_weight.permute(1, 2, 0)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff).permute(1, 0, 2)\n",
        "\n",
        "        # Sort input data and adaptively update grid\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[\n",
        "            torch.linspace(0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device)\n",
        "        ]\n",
        "\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (\n",
        "            torch.arange(self.grid_size + 1, dtype=torch.float32, device=x.device).unsqueeze(1)\n",
        "            * uniform_step\n",
        "            + x_sorted[0]\n",
        "            - margin\n",
        "        )\n",
        "\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.cat(\n",
        "            [\n",
        "                grid[:1]\n",
        "                - uniform_step\n",
        "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
        "                grid,\n",
        "                grid[-1:]\n",
        "                + uniform_step\n",
        "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Update grid and spline weights with new grid values\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        \"\"\"\n",
        "        Compute regularization loss based on activation and entropy.\n",
        "\n",
        "        Args:\n",
        "            regularize_activation (float, optional): Weight for activation regularization loss. Default is 1.0.\n",
        "            regularize_entropy (float, optional): Weight for entropy regularization loss. Default is 1.0.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Regularization loss value.\n",
        "        \"\"\"\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return (\n",
        "            regularize_activation * regularization_loss_activation\n",
        "            + regularize_entropy * regularization_loss_entropy\n",
        "        )\n"
      ],
      "metadata": {
        "id": "U5hPCg3ljo3o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define KAN Class\n",
        "class KAN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network model implementing the Kolmogorov-Arnold Neural Network (KAN) architecture.\n",
        "    This model uses a series of KANLinear layers to build a deep network with B-spline basis functions.\n",
        "\n",
        "    Parameters:\n",
        "    - layers_hidden (list of int): List defining the number of units in each hidden layer.\n",
        "      The input layer size is the first element, and the output layer size is the last element.\n",
        "    - grid_size (int, optional): Number of grid points for the B-spline basis functions. Default is 5.\n",
        "    - spline_order (int, optional): Order of the B-spline basis functions. Default is 3.\n",
        "    - scale_noise (float, optional): Scale of the noise added to the spline weights. Default is 0.1.\n",
        "    - scale_base (float, optional): Scaling factor for the base weights. Default is 1.0.\n",
        "    - scale_spline (float, optional): Scaling factor for the spline weights. Default is 1.0.\n",
        "    - base_activation (torch.nn.Module, optional): Activation function applied to the base weights. Default is SiLU.\n",
        "    - grid_eps (float, optional): Mixing factor for adaptive and uniform grid adjustment. Default is 0.02.\n",
        "    - grid_range (list of float, optional): Range of the grid for the B-spline basis functions. Default is [-1, 1].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers_hidden,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KAN, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        # Create a list of KANLinear layers based on the specified architecture\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
        "            self.layers.append(\n",
        "                KANLinear(\n",
        "                    in_features,\n",
        "                    out_features,\n",
        "                    grid_size=grid_size,\n",
        "                    spline_order=spline_order,\n",
        "                    scale_noise=scale_noise,\n",
        "                    scale_base=scale_base,\n",
        "                    scale_spline=scale_spline,\n",
        "                    base_activation=base_activation,\n",
        "                    grid_eps=grid_eps,\n",
        "                    grid_range=grid_range,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, update_grid=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): Input tensor.\n",
        "        - update_grid (bool, optional): Whether to update the grid based on the current input.\n",
        "          Default is False.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor after passing through all layers.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            if update_grid:\n",
        "                layer.update_grid(x)\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        \"\"\"\n",
        "        Compute the regularization loss for the network.\n",
        "\n",
        "        Parameters:\n",
        "        - regularize_activation (float, optional): Weight for the activation regularization term. Default is 1.0.\n",
        "        - regularize_entropy (float, optional): Weight for the entropy regularization term. Default is 1.0.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Regularization loss.\n",
        "        \"\"\"\n",
        "        return sum(\n",
        "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
        "            for layer in self.layers\n",
        "        )\n"
      ],
      "metadata": {
        "id": "3g7hMjsclb-Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define transformations for the training and validation datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                     # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))       # Normalize the images with mean=0.5 and std=0.5\n",
        "])\n",
        "\n",
        "# Load the MNIST training dataset with the specified transformations\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",                              # Directory where the data will be stored\n",
        "    train=True,                                 # Download the training set\n",
        "    download=True,                              # Download the data if not already present\n",
        "    transform=transform                         # Apply the defined transformations\n",
        ")\n",
        "\n",
        "# Load the MNIST validation dataset with the specified transformations\n",
        "valset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",                              # Directory where the data will be stored\n",
        "    train=False,                                # Download the test set (validation)\n",
        "    download=True,                              # Download the data if not already present\n",
        "    transform=transform                         # Apply the defined transformations\n",
        ")\n",
        "\n",
        "# Create data loaders for training and validation datasets\n",
        "trainloader = DataLoader(\n",
        "    trainset,                                   # Dataset to load\n",
        "    batch_size=64,                              # Number of samples per batch\n",
        "    shuffle=True                                # Shuffle the data at every epoch\n",
        ")\n",
        "\n",
        "valloader = DataLoader(\n",
        "    valset,                                     # Dataset to load\n",
        "    batch_size=64,                              # Number of samples per batch\n",
        "    shuffle=False                               # Do not shuffle the data (useful for validation)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ghbnt7kkjo0v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the KAN model\n",
        "# The model expects an input size of 28x28 (flattened), a hidden layer of 64 neurons, and an output layer of 10 classes (for MNIST digits 0-9)\n",
        "model = KAN([28 * 28, 64, 10])\n",
        "\n",
        "# Check if a GPU is available and move the model to the appropriate device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "# We use the AdamW optimizer, which is a variant of the Adam optimizer with weight decay (L2 regularization) included\n",
        "# - `model.parameters()` provides the model's parameters to the optimizer\n",
        "# - `lr=1e-3` sets the initial learning rate\n",
        "# - `weight_decay=1e-4` adds regularization to prevent overfitting\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "# We use an ExponentialLR scheduler, which reduces the learning rate by a factor of `gamma` after each epoch\n",
        "# - `optimizer` is the optimizer whose learning rate is adjusted\n",
        "# - `gamma=0.8` reduces the learning rate by 20% each epoch\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
        "\n",
        "# Define the loss function\n",
        "# CrossEntropyLoss is suitable for multi-class classification problems like MNIST\n",
        "# It combines `nn.LogSoftmax()` and `nn.NLLLoss()` in a single class\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "l8g_WhCPjoyC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training loop for 5 epochs\n",
        "for epoch in range(5):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize a progress bar for the training loop using tqdm\n",
        "    with tqdm(trainloader) as pbar:\n",
        "        for i, (images, labels) in enumerate(pbar):\n",
        "            # Flatten the images and move them to the appropriate device\n",
        "            images = images.view(-1, 28 * 28).to(device)\n",
        "\n",
        "            # Zero the gradients before the backward pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            output = model(images)\n",
        "\n",
        "            # Compute the loss between the output and the true labels\n",
        "            loss = criterion(output, labels.to(device))\n",
        "\n",
        "            # Backward pass: compute the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model parameters based on the gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate accuracy for the current batch\n",
        "            accuracy = (output.argmax(dim=1) == labels.to(device)).float().mean()\n",
        "\n",
        "            # Update the progress bar with the current loss, accuracy, and learning rate\n",
        "            pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode (disables dropout, batchnorm, etc.)\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "\n",
        "    # Disable gradient calculation for validation to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valloader:\n",
        "            # Flatten the images and move them to the appropriate device\n",
        "            images = images.view(-1, 28 * 28).to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            output = model(images)\n",
        "\n",
        "            # Accumulate the validation loss\n",
        "            val_loss += criterion(output, labels.to(device)).item()\n",
        "\n",
        "            # Accumulate the validation accuracy\n",
        "            val_accuracy += (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
        "\n",
        "    # Calculate the average validation loss and accuracy\n",
        "    val_loss /= len(valloader)\n",
        "    val_accuracy /= len(valloader)\n",
        "\n",
        "    # Update the learning rate according to the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print the results for the current epoch\n",
        "    print(f\"Epoch {epoch + 1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm_m_5wskJPu",
        "outputId": "9c8e499a-9b5a-442a-8989-c6cf9af3e9df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:53<00:00, 17.63it/s, accuracy=0.938, loss=0.21, lr=0.001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Loss: 0.2147, Val Accuracy: 0.9358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:53<00:00, 17.44it/s, accuracy=0.969, loss=0.174, lr=0.0008]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Val Loss: 0.1580, Val Accuracy: 0.9538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:57<00:00, 16.22it/s, accuracy=0.969, loss=0.194, lr=0.00064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Val Loss: 0.1239, Val Accuracy: 0.9625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:55<00:00, 16.97it/s, accuracy=1, loss=0.0188, lr=0.000512]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Val Loss: 0.1121, Val Accuracy: 0.9679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:58<00:00, 16.09it/s, accuracy=1, loss=0.0133, lr=0.00041]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Val Loss: 0.1052, Val Accuracy: 0.9690\n"
          ]
        }
      ]
    }
  ]
}